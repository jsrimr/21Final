{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Final Project: Text-to-image synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the R-precision score **</font> so that TAs can grade the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys, os, pickle\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append('..')\n",
    "from evaluation.model import CNN_ENCODER, RNN_ENCODER\n",
    "from utils.data_utils import CUBDataset\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = torch.device('cpu' if not torch.cuda.is_available() else 'cuda')\n",
    "\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "cfg_from_file('../cfg/train_birds.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Returns cosine similarity between x1 and x2, computed along dim\n",
    "    \"\"\"\n",
    "    w12 = torch.sum(x1 * x2, dim)\n",
    "    w1 = torch.norm(x1, 2, dim)\n",
    "    w2 = torch.norm(x2, 2, dim)\n",
    "    return (w12 / (w1 * w2).clamp(min=eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(imgs, captions, captions_lens, class_ids, keys, captions_idx):\n",
    "    # sort data by the length in a decreasing order\n",
    "    # the reason of sorting data can be found in https://simonjisu.github.io/nlp/2018/07/05/packedsequence.html \n",
    "    sorted_cap_lens, sorted_cap_indices = torch.sort(captions_lens, 0, True)\n",
    "    real_imgs = []\n",
    "    for i in range(len(imgs)):\n",
    "        imgs[i] = imgs[i][sorted_cap_indices]\n",
    "        real_imgs.append(imgs[i])\n",
    "\n",
    "    sorted_captions = captions[sorted_cap_indices].squeeze()\n",
    "    if captions.size(0) == 1:\n",
    "        captions = captions.unsqueeze(0)\n",
    "    sorted_class_ids = class_ids[sorted_cap_indices].numpy()\n",
    "    sorted_keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
    "    sorted_captions_idx = captions_idx[sorted_cap_indices].numpy()\n",
    "\n",
    "    return [real_imgs, sorted_captions, sorted_cap_lens, sorted_class_ids, sorted_keys, sorted_captions_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load image encoder\n",
      "Load text encoder\n"
     ]
    }
   ],
   "source": [
    "image_encoder = CNN_ENCODER(256)\n",
    "state_dict = torch.load('./sim_models/bird/image_encoder.pth', map_location=lambda storage, loc: storage)\n",
    "image_encoder.load_state_dict(state_dict)\n",
    "for p in image_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "print('Load image encoder')\n",
    "image_encoder.eval()\n",
    "\n",
    "# load the image encoder model to obtain the latent feature of the real caption\n",
    "text_encoder = RNN_ENCODER(5450, nhidden=256)\n",
    "state_dict = torch.load('./sim_models/bird/text_encoder.pth', map_location=lambda storage, loc: storage)\n",
    "text_encoder.load_state_dict(state_dict)\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "print('Load text encoder')\n",
    "text_encoder.eval()\n",
    "\n",
    "image_encoder = image_encoder.to(device)\n",
    "text_encoder = text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_dir:\n",
      "/home/jeffrey/21Final/evaluation\n",
      "\n",
      "self.data_dir:\n",
      "/home/jeffrey/21Final/data/birds\n",
      "\n",
      "self.image_dir:\n",
      "/home/jeffrey/21Final/data/birds/CUB-200-2011/images\n",
      "\n",
      "filepath /home/jeffrey/21Final/data/birds/captions.pickle\n",
      "Load from:  /home/jeffrey/21Final/data/birds/captions.pickle\n",
      "\ttest data directory:\n",
      "/home/jeffrey/21Final/data/birds/test\n",
      "\n",
      "\t# of test filenames:(2933,)\n",
      "\n",
      "\texample of filename of test image:001.Black_footed_Albatross/Black_Footed_Albatross_0046_18\n",
      "\n",
      "\texample of caption and its ids:\n",
      "['this', 'is', 'a', 'small', 'bird', 'that', 'has', 'a', 'brilliant', 'blue', 'color', 'on', 'it', 's', 'body', 'a', 'slightly', 'darker', 'blue', 'on', 'it', 's', 'head', 'a', 'teal', 'color', 'on', 'it', 's', 'wings', 'and', 'a', 'light', 'colored', 'beak']\n",
      "[18, 19, 1, 250, 2, 33, 13, 1, 853, 50, 37, 86, 53, 54, 15, 1, 178, 31, 50, 86, 53, 54, 25, 1, 1054, 37, 86, 53, 54, 17, 8, 1, 67, 89, 10]\n",
      "\n",
      "\t# of test captions:(29330,)\n",
      "\n",
      "\t# of test caption ids:(29330,)\n",
      "\n",
      "self.current_dir:\n",
      "/home/jeffrey/21Final/evaluation\n",
      "\n",
      "self.data_dir:\n",
      "/home/jeffrey/21Final/data/birds\n",
      "\n",
      "self.image_dir:\n",
      "/home/jeffrey/21Final/data/birds/CUB-200-2011/images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147529/2847039090.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(f'\\t# of test captions:{np.asarray(test_dataset.captions).shape}\\n')\n",
      "/tmp/ipykernel_147529/2847039090.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(f'\\t# of test caption ids:{np.asarray(test_dataset.captions_ids).shape}\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath /home/jeffrey/21Final/data/birds/captions.pickle\n",
      "Load from:  /home/jeffrey/21Final/data/birds/captions.pickle\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128))\n",
    "])\n",
    "\n",
    "test_dataset = CUBDataset(cfg.DATA_DIR, transform=transform, split='test', eval_mode=True)\n",
    "\n",
    "print(f'\\ttest data directory:\\n{test_dataset.split_dir}\\n')\n",
    "print(f'\\t# of test filenames:{test_dataset.filenames.shape}\\n')\n",
    "print(f'\\texample of filename of test image:{test_dataset.filenames[0]}\\n')\n",
    "print(f'\\texample of caption and its ids:\\n{test_dataset.captions[0]}\\n{test_dataset.captions_ids[0]}\\n')\n",
    "print(f'\\t# of test captions:{np.asarray(test_dataset.captions).shape}\\n')\n",
    "print(f'\\t# of test caption ids:{np.asarray(test_dataset.captions_ids).shape}\\n')\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                                              drop_last=False, shuffle=False, num_workers=int(cfg.WORKERS))\n",
    "\n",
    "test_dataset_for_wrong = CUBDataset(cfg.DATA_DIR, transform=transform, split='test', eval_mode=True, for_wrong=True)\n",
    "current_dir = os.getcwd()\n",
    "fname = os.path.join(current_dir.replace('/evaluation', ''), cfg.DATA_DIR, 'test', 'test_caption_info.pickle')\n",
    "with open(fname, 'rb') as f:\n",
    "    test_caption_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147529/3691870953.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  n_data = np.asarray(test_dataset.captions_ids).shape[0]\n"
     ]
    }
   ],
   "source": [
    "n_data = np.asarray(test_dataset.captions_ids).shape[0]\n",
    "true_cnn_features = np.zeros((n_data, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "true_rnn_features = np.zeros((n_data, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "wrong_rnn_features = np.zeros((n_data, cfg.WRONG_CAPTION, cfg.TEXT.EMBEDDING_DIM), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffrey/anaconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/jeffrey/anaconda3/envs/gpu/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jeffrey/anaconda3/envs/gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jeffrey/anaconda3/envs/gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jeffrey/21Final/evaluation/../utils/data_utils.py\", line 361, in __getitem__\n    gen_imgs = get_imgs(gen_img_name, self.imsize, bbox=None, transform=self.transform, normalize=self.norm)\n  File \"/home/jeffrey/21Final/evaluation/../utils/data_utils.py\", line 20, in get_imgs\n    img = Image.open(img_path).convert('RGB')\n  File \"/home/jeffrey/anaconda3/envs/gpu/lib/python3.8/site-packages/PIL/Image.py\", line 2975, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/home/jeffrey/21Final/./evaluation/generated_images/004.Groove_billed_Ani/Groove_Billed_Ani_0019_1585_0.png'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_147529/2504838053.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtotal_cs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcnn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gen_img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpu/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/jeffrey/anaconda3/envs/gpu/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jeffrey/anaconda3/envs/gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jeffrey/anaconda3/envs/gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jeffrey/21Final/evaluation/../utils/data_utils.py\", line 361, in __getitem__\n    gen_imgs = get_imgs(gen_img_name, self.imsize, bbox=None, transform=self.transform, normalize=self.norm)\n  File \"/home/jeffrey/21Final/evaluation/../utils/data_utils.py\", line 20, in get_imgs\n    img = Image.open(img_path).convert('RGB')\n  File \"/home/jeffrey/anaconda3/envs/gpu/lib/python3.8/site-packages/PIL/Image.py\", line 2975, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/home/jeffrey/21Final/./evaluation/generated_images/004.Groove_billed_Ani/Groove_Billed_Ani_0019_1585_0.png'\n"
     ]
    }
   ],
   "source": [
    "total_cs = []\n",
    "cnn_features = []\n",
    "for batch_idx, data in enumerate(test_dataloader):\n",
    "    gen_imgs = data['gen_img']\n",
    "    captions = data['caps']\n",
    "    captions_lens = data['cap_len']\n",
    "    class_ids = data['cls_id']\n",
    "    keys = data['key']\n",
    "    captions_idx = data['cap_ix']\n",
    "    \n",
    "    sorted_gen_imgs, sorted_captions, sorted_cap_lens, sorted_class_ids, sorted_keys, sorted_captions_idx = prepare_data(gen_imgs, captions, captions_lens, class_ids, keys, captions_idx)\n",
    "    \n",
    "    if cfg.CUDA:\n",
    "        sorted_captions = sorted_captions.to(device)\n",
    "        sorted_cap_lens = sorted_cap_lens.to(device)\n",
    "    \n",
    "    hidden = text_encoder.init_hidden(sorted_captions.size(0))\n",
    "    _, sent_emb = text_encoder(sorted_captions, sorted_cap_lens, hidden)\n",
    "\n",
    "    _, gen_sent_code = image_encoder(sorted_gen_imgs[-1].to(device))\n",
    "\n",
    "    true_sim = cosine_similarity(gen_sent_code, sent_emb)\n",
    "    \n",
    "    true_cnn_features[captions_idx] = gen_sent_code.detach().cpu().numpy()\n",
    "    true_rnn_features[captions_idx] = sent_emb.detach().cpu().numpy()\n",
    "    \n",
    "    false_sim_list = []\n",
    "    for i in range(captions.size(0)):\n",
    "        assert sorted_class_ids[i] == test_caption_info[sorted_captions_idx[i]][0]\n",
    "        cap_cls_id = test_caption_info[sorted_captions_idx[i]][1]\n",
    "        mis_match_captions, sorted_mis_cap_lens = test_dataset_for_wrong.get_mis_captions(sorted_class_ids[i], cap_cls_id)\n",
    "        \n",
    "        if cfg.CUDA:\n",
    "            mis_match_captions = mis_match_captions.to(device)\n",
    "            sorted_mis_cap_lens = sorted_mis_cap_lens.to(device)\n",
    "        \n",
    "        mis_hidden = text_encoder.init_hidden(mis_match_captions.size(0))\n",
    "        _, mis_sent_emb = text_encoder(mis_match_captions, sorted_mis_cap_lens, mis_hidden)\n",
    "\n",
    "        false_sim = cosine_similarity(gen_sent_code[i:i+1], mis_sent_emb)\n",
    "        \n",
    "        wrong_rnn_features[captions_idx] = mis_sent_emb.detach().cpu().numpy()\n",
    "        \n",
    "        false_sim_list.append(false_sim)\n",
    "    \n",
    "    batch_cs = torch.cat([torch.unsqueeze(true_sim, 1), torch.stack(false_sim_list, dim=0)], dim=1)\n",
    "    total_cs.append(batch_cs)\n",
    "\n",
    "total_cs = torch.cat(total_cs, dim=0)\n",
    "r_precision = torch.mean((torch.argmax(total_cs, dim=1) == 0) * 1.0).cpu().detach().numpy()\n",
    "print('# images for evaluation:', len(total_cs))\n",
    "print('R-precision: ' + str(r_precision))\n",
    "\n",
    "np.savez(cfg.R_PRECISION_FILE, total_cs=total_cs.detach().cpu().numpy(),\n",
    "         true_cnn_features=true_cnn_features, true_rnn_features=true_rnn_features, wrong_rnn_features=wrong_rnn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
