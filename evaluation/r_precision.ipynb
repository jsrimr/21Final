{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Final Project: Text-to-image synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the R-precision score **</font> so that TAs can grade the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys, os, pickle\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append('..')\n",
    "from evaluation.model import CNN_ENCODER, RNN_ENCODER\n",
    "from utils.data_utils import CUBDataset\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = torch.device('cpu' if not torch.cuda.is_available() else 'cuda')\n",
    "\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "cfg_from_file('../cfg/train_birds.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Returns cosine similarity between x1 and x2, computed along dim\n",
    "    \"\"\"\n",
    "    w12 = torch.sum(x1 * x2, dim)\n",
    "    w1 = torch.norm(x1, 2, dim)\n",
    "    w2 = torch.norm(x2, 2, dim)\n",
    "    return (w12 / (w1 * w2).clamp(min=eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(imgs, captions, captions_lens, class_ids, keys, captions_idx):\n",
    "    # sort data by the length in a decreasing order\n",
    "    # the reason of sorting data can be found in https://simonjisu.github.io/nlp/2018/07/05/packedsequence.html \n",
    "    sorted_cap_lens, sorted_cap_indices = torch.sort(captions_lens, 0, True)\n",
    "    real_imgs = []\n",
    "    for i in range(len(imgs)):\n",
    "        imgs[i] = imgs[i][sorted_cap_indices]\n",
    "        real_imgs.append(imgs[i])\n",
    "\n",
    "    sorted_captions = captions[sorted_cap_indices].squeeze()\n",
    "    if captions.size(0) == 1:\n",
    "        captions = captions.unsqueeze(0)\n",
    "    sorted_class_ids = class_ids[sorted_cap_indices].numpy()\n",
    "    sorted_keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
    "    sorted_captions_idx = captions_idx[sorted_cap_indices].numpy()\n",
    "\n",
    "    return [real_imgs, sorted_captions, sorted_cap_lens, sorted_class_ids, sorted_keys, sorted_captions_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = CNN_ENCODER(256)\n",
    "state_dict = torch.load('./sim_models/bird/image_encoder.pth', map_location=lambda storage, loc: storage)\n",
    "image_encoder.load_state_dict(state_dict)\n",
    "for p in image_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "print('Load image encoder')\n",
    "image_encoder.eval()\n",
    "\n",
    "# load the image encoder model to obtain the latent feature of the real caption\n",
    "text_encoder = RNN_ENCODER(5450, nhidden=256)\n",
    "state_dict = torch.load('./sim_models/bird/text_encoder.pth', map_location=lambda storage, loc: storage)\n",
    "text_encoder.load_state_dict(state_dict)\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "print('Load text encoder')\n",
    "text_encoder.eval()\n",
    "\n",
    "image_encoder = image_encoder.to(device)\n",
    "text_encoder = text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128))\n",
    "])\n",
    "\n",
    "test_dataset = CUBDataset(cfg.DATA_DIR, transform=transform, split='test', eval_mode=True)\n",
    "\n",
    "print(f'\\ttest data directory:\\n{test_dataset.split_dir}\\n')\n",
    "print(f'\\t# of test filenames:{test_dataset.filenames.shape}\\n')\n",
    "print(f'\\texample of filename of test image:{test_dataset.filenames[0]}\\n')\n",
    "print(f'\\texample of caption and its ids:\\n{test_dataset.captions[0]}\\n{test_dataset.captions_ids[0]}\\n')\n",
    "print(f'\\t# of test captions:{np.asarray(test_dataset.captions).shape}\\n')\n",
    "print(f'\\t# of test caption ids:{np.asarray(test_dataset.captions_ids).shape}\\n')\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                                              drop_last=False, shuffle=False, num_workers=int(cfg.WORKERS))\n",
    "\n",
    "test_dataset_for_wrong = CUBDataset(cfg.DATA_DIR, transform=transform, split='test', eval_mode=True, for_wrong=True)\n",
    "current_dir = os.getcwd()\n",
    "fname = os.path.join(current_dir.replace('/evaluation', ''), cfg.DATA_DIR, 'test', 'test_caption_info.pickle')\n",
    "with open(fname, 'rb') as f:\n",
    "    test_caption_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = np.asarray(test_dataset.captions_ids).shape[0]\n",
    "true_cnn_features = np.zeros((n_data, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "true_rnn_features = np.zeros((n_data, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "wrong_rnn_features = np.zeros((n_data, cfg.WRONG_CAPTION, cfg.TEXT.EMBEDDING_DIM), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cs = []\n",
    "cnn_features = []\n",
    "for batch_idx, data in enumerate(test_dataloader):\n",
    "    gen_imgs = data['gen_img']\n",
    "    captions = data['caps']\n",
    "    captions_lens = data['cap_len']\n",
    "    class_ids = data['cls_id']\n",
    "    keys = data['key']\n",
    "    captions_idx = data['cap_ix']\n",
    "    \n",
    "    sorted_gen_imgs, sorted_captions, sorted_cap_lens, sorted_class_ids, sorted_keys, sorted_captions_idx = prepare_data(gen_imgs, captions, captions_lens, class_ids, keys, captions_idx)\n",
    "    \n",
    "    if cfg.CUDA:\n",
    "        sorted_captions = sorted_captions.to(device)\n",
    "        sorted_cap_lens = sorted_cap_lens.to(device)\n",
    "    \n",
    "    hidden = text_encoder.init_hidden(sorted_captions.size(0))\n",
    "    _, sent_emb = text_encoder(sorted_captions, sorted_cap_lens, hidden)\n",
    "\n",
    "    _, gen_sent_code = image_encoder(sorted_gen_imgs[-1].to(device))\n",
    "\n",
    "    true_sim = cosine_similarity(gen_sent_code, sent_emb)\n",
    "    \n",
    "    true_cnn_features[captions_idx] = gen_sent_code.detach().cpu().numpy()\n",
    "    true_rnn_features[captions_idx] = sent_emb.detach().cpu().numpy()\n",
    "    \n",
    "    false_sim_list = []\n",
    "    for i in range(captions.size(0)):\n",
    "        assert sorted_class_ids[i] == test_caption_info[sorted_captions_idx[i]][0]\n",
    "        cap_cls_id = test_caption_info[sorted_captions_idx[i]][1]\n",
    "        mis_match_captions, sorted_mis_cap_lens = test_dataset_for_wrong.get_mis_captions(sorted_class_ids[i], cap_cls_id)\n",
    "        \n",
    "        if cfg.CUDA:\n",
    "            mis_match_captions = mis_match_captions.to(device)\n",
    "            sorted_mis_cap_lens = sorted_mis_cap_lens.to(device)\n",
    "        \n",
    "        mis_hidden = text_encoder.init_hidden(mis_match_captions.size(0))\n",
    "        _, mis_sent_emb = text_encoder(mis_match_captions, sorted_mis_cap_lens, mis_hidden)\n",
    "\n",
    "        false_sim = cosine_similarity(gen_sent_code[i:i+1], mis_sent_emb)\n",
    "        \n",
    "        wrong_rnn_features[captions_idx] = mis_sent_emb.detach().cpu().numpy()\n",
    "        \n",
    "        false_sim_list.append(false_sim)\n",
    "    \n",
    "    batch_cs = torch.cat([torch.unsqueeze(true_sim, 1), torch.stack(false_sim_list, dim=0)], dim=1)\n",
    "    total_cs.append(batch_cs)\n",
    "\n",
    "total_cs = torch.cat(total_cs, dim=0)\n",
    "r_precision = torch.mean((torch.argmax(total_cs, dim=1) == 0) * 1.0).cpu().detach().numpy()\n",
    "print('# images for evaluation:', len(total_cs))\n",
    "print('R-precision: ' + str(r_precision))\n",
    "\n",
    "np.savez(cfg.R_PRECISION_FILE, total_cs=total_cs.detach().cpu().numpy(),\n",
    "         true_cnn_features=true_cnn_features, true_rnn_features=true_rnn_features, wrong_rnn_features=wrong_rnn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
